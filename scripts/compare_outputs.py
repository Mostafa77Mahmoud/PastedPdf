
#!/usr/bin/env python3
"""
Ù…Ù‚Ø§Ø±Ù†Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù…Ù„ÙØ§Øª PDF Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
ÙŠØ­Ù„Ù„ Ø§Ù„ÙØ±ÙˆÙ‚Ø§Øª ÙˆÙŠÙˆØµÙŠ Ø¨Ø£ÙØ¶Ù„ Ù…Ù„Ù Ù„Ù€ Gemini File Search
"""

import sys
from pathlib import Path
import fitz  # PyMuPDF
import json
from collections import defaultdict

sys.path.insert(0, str(Path(__file__).parent.parent))

def analyze_pdf(pdf_path):
    """ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù…Ù„Ù PDF"""
    try:
        doc = fitz.open(pdf_path)
        
        analysis = {
            'file_name': Path(pdf_path).name,
            'file_size_mb': Path(pdf_path).stat().st_size / (1024 * 1024),
            'total_pages': len(doc),
            'total_text_length': 0,
            'searchable_pages': 0,
            'empty_pages': 0,
            'pages_with_images': 0,
            'total_images': 0,
            'average_text_per_page': 0,
            'text_samples': []
        }
        
        text_lengths = []
        
        for page_num in range(len(doc)):
            page = doc[page_num]
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ
            text = page.get_text("text")
            text_length = len(text.strip())
            text_lengths.append(text_length)
            
            analysis['total_text_length'] += text_length
            
            if text_length > 50:
                analysis['searchable_pages'] += 1
            
            if text_length < 10:
                analysis['empty_pages'] += 1
            
            # Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ±
            images = page.get_images()
            if images:
                analysis['pages_with_images'] += 1
                analysis['total_images'] += len(images)
            
            # Ø¹ÙŠÙ†Ø§Øª Ù†ØµÙŠØ© Ù…Ù† ØµÙØ­Ø§Øª Ù…Ø®ØªÙ„ÙØ©
            if page_num in [0, len(doc)//4, len(doc)//2, 3*len(doc)//4, len(doc)-1]:
                sample = text[:500] if text else "(ØµÙØ­Ø© ÙØ§Ø±ØºØ©)"
                analysis['text_samples'].append({
                    'page': page_num + 1,
                    'text': sample
                })
        
        if analysis['total_pages'] > 0:
            analysis['average_text_per_page'] = analysis['total_text_length'] / analysis['total_pages']
        
        # Ù†Ø³Ø¨ Ù…Ø¦ÙˆÙŠØ©
        analysis['searchable_percentage'] = (analysis['searchable_pages'] / analysis['total_pages']) * 100
        analysis['empty_percentage'] = (analysis['empty_pages'] / analysis['total_pages']) * 100
        
        doc.close()
        return analysis
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ {pdf_path}: {e}")
        return None

def analyze_text_file(text_path):
    """ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù Ù†ØµÙŠ"""
    try:
        with open(text_path, 'r', encoding='utf-8') as f:
            text = f.read()
        
        lines = text.split('\n')
        
        analysis = {
            'file_name': Path(text_path).name,
            'file_size_mb': Path(text_path).stat().st_size / (1024 * 1024),
            'total_characters': len(text),
            'total_lines': len(lines),
            'non_empty_lines': len([l for l in lines if l.strip()]),
            'arabic_chars': sum(1 for c in text if '\u0600' <= c <= '\u06FF'),
            'english_chars': sum(1 for c in text if c.isalpha() and c.isascii()),
            'digits': sum(1 for c in text if c.isdigit()),
            'sample': text[:1000]
        }
        
        # Ù†Ø³Ø¨Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
        total_letters = analysis['arabic_chars'] + analysis['english_chars']
        if total_letters > 0:
            analysis['arabic_percentage'] = (analysis['arabic_chars'] / total_letters) * 100
        else:
            analysis['arabic_percentage'] = 0
        
        return analysis
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ {text_path}: {e}")
        return None

def compare_pdfs(analyses):
    """Ù…Ù‚Ø§Ø±Ù†Ø© ØªÙØµÙŠÙ„ÙŠØ© Ø¨ÙŠÙ† Ù…Ù„ÙØ§Øª PDF"""
    print("\n" + "="*70)
    print("Ù…Ù‚Ø§Ø±Ù†Ø© ØªÙØµÙŠÙ„ÙŠØ© Ø¨ÙŠÙ† Ù…Ù„ÙØ§Øª PDF")
    print("="*70)
    
    for key, analysis in analyses.items():
        if analysis:
            print(f"\nğŸ“„ {analysis['file_name']}")
            print(f"   Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù: {analysis['file_size_mb']:.2f} MB")
            print(f"   Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª: {analysis['total_pages']}")
            print(f"   ØµÙØ­Ø§Øª Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø¨Ø­Ø«: {analysis['searchable_pages']} ({analysis['searchable_percentage']:.1f}%)")
            print(f"   ØµÙØ­Ø§Øª ÙØ§Ø±ØºØ©: {analysis['empty_pages']} ({analysis['empty_percentage']:.1f}%)")
            print(f"   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù†Øµ: {analysis['total_text_length']:,} Ø­Ø±Ù")
            print(f"   Ù…ØªÙˆØ³Ø· Ø§Ù„Ù†Øµ/ØµÙØ­Ø©: {analysis['average_text_per_page']:.0f} Ø­Ø±Ù")
            print(f"   Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ±: {analysis['total_images']}")

def recommend_best_file(pdf_analyses, text_analyses):
    """Ø§Ù„ØªÙˆØµÙŠØ© Ø¨Ø£ÙØ¶Ù„ Ù…Ù„Ù Ù„Ù€ Gemini File Search"""
    print("\n" + "="*70)
    print("ğŸ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ù„Ù€ Gemini File Search")
    print("="*70)
    
    scores = {}
    
    # ØªØ³Ø¬ÙŠÙ„ Ù…Ù„ÙØ§Øª PDF
    for key, analysis in pdf_analyses.items():
        if not analysis:
            continue
        
        score = 0
        reasons = []
        
        # Ù†Ù‚Ø§Ø· Ù„Ù„Ù†Øµ Ø§Ù„Ù‚Ø§Ø¨Ù„ Ù„Ù„Ø¨Ø­Ø«
        if analysis['searchable_percentage'] > 95:
            score += 40
            reasons.append("âœ… Ù†Øµ Ù‚Ø§Ø¨Ù„ Ù„Ù„Ø¨Ø­Ø« Ø¨Ù†Ø³Ø¨Ø© Ø¹Ø§Ù„ÙŠØ©")
        elif analysis['searchable_percentage'] > 80:
            score += 30
            reasons.append("âœ“ Ù†Øµ Ù‚Ø§Ø¨Ù„ Ù„Ù„Ø¨Ø­Ø« Ø¨Ù†Ø³Ø¨Ø© Ø¬ÙŠØ¯Ø©")
        else:
            score += 10
            reasons.append("âš ï¸ Ù†Øµ Ù‚Ø§Ø¨Ù„ Ù„Ù„Ø¨Ø­Ø« Ø¨Ù†Ø³Ø¨Ø© Ù…Ù†Ø®ÙØ¶Ø©")
        
        # Ù†Ù‚Ø§Ø· Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ù†Øµ ÙÙŠ Ø§Ù„ØµÙØ­Ø©
        if analysis['average_text_per_page'] > 500:
            score += 30
            reasons.append("âœ… Ù…Ø­ØªÙˆÙ‰ Ù†ØµÙŠ ØºÙ†ÙŠ")
        elif analysis['average_text_per_page'] > 200:
            score += 20
            reasons.append("âœ“ Ù…Ø­ØªÙˆÙ‰ Ù†ØµÙŠ Ù…Ø¹ØªØ¯Ù„")
        
        # Ø®ØµÙ… Ù„Ù„ØµÙØ­Ø§Øª Ø§Ù„ÙØ§Ø±ØºØ©
        if analysis['empty_percentage'] < 5:
            score += 20
            reasons.append("âœ… ØµÙØ­Ø§Øª ÙØ§Ø±ØºØ© Ù‚Ù„ÙŠÙ„Ø©")
        elif analysis['empty_percentage'] < 10:
            score += 10
        else:
            score -= 10
            reasons.append("âš ï¸ Ø¹Ø¯Ø¯ ØµÙØ­Ø§Øª ÙØ§Ø±ØºØ© ÙƒØ¨ÙŠØ±")
        
        # Ù†Ù‚Ø§Ø· Ù„Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù (Ø£ØµØºØ± Ø£ÙØ¶Ù„ Ù„Ù„Ø±ÙØ¹)
        if analysis['file_size_mb'] < 50:
            score += 10
            reasons.append("âœ“ Ø­Ø¬Ù… Ù…Ù„Ù Ù…Ù†Ø§Ø³Ø¨")
        
        scores[key] = {'score': score, 'reasons': reasons, 'analysis': analysis}
    
    # ØªØ³Ø¬ÙŠÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Øµ
    for key, analysis in text_analyses.items():
        if not analysis:
            continue
        
        score = 0
        reasons = []
        
        # Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Øµ ØªØ­ØµÙ„ Ø¹Ù„Ù‰ Ù†Ù‚Ø§Ø· Ø¹Ø§Ù„ÙŠØ© Ù„Ù„Ø¨Ø­Ø«
        score += 50
        reasons.append("âœ… Ù†Øµ Ø®Ø§Ù… - Ø¨Ø­Ø« Ù…Ø¨Ø§Ø´Ø± ÙˆØ³Ø±ÙŠØ¹")
        
        # Ù†Ù‚Ø§Ø· Ù„Ù„Ù…Ø­ØªÙˆÙ‰
        if analysis['total_characters'] > 100000:
            score += 30
            reasons.append("âœ… Ù…Ø­ØªÙˆÙ‰ Ù†ØµÙŠ Ø´Ø§Ù…Ù„")
        
        # Ù†Ù‚Ø§Ø· Ù„Ù„Ø£Ø³Ø·Ø± ØºÙŠØ± Ø§Ù„ÙØ§Ø±ØºØ©
        if analysis['non_empty_lines'] > 1000:
            score += 20
            reasons.append("âœ… Ø¹Ø¯Ø¯ Ø£Ø³Ø·Ø± ÙƒØ¨ÙŠØ±")
        
        scores[key] = {'score': score, 'reasons': reasons, 'analysis': analysis}
    
    # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø·
    sorted_scores = sorted(scores.items(), key=lambda x: x[1]['score'], reverse=True)
    
    print("\nğŸ† Ø§Ù„ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ù…Ù„Ø§Ø¡Ù…Ø© Ù„Ù€ Gemini File Search:")
    print()
    
    for i, (key, data) in enumerate(sorted_scores, 1):
        print(f"{i}. {data['analysis']['file_name']}")
        print(f"   Ø§Ù„Ù†Ù‚Ø§Ø·: {data['score']}/100")
        for reason in data['reasons']:
            print(f"   {reason}")
        print()
    
    # Ø§Ù„ØªÙˆØµÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    best = sorted_scores[0]
    print("="*70)
    print("ğŸ¯ Ø§Ù„ØªÙˆØµÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
    print(f"   Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø£ÙØ¶Ù„: {best[1]['analysis']['file_name']}")
    print(f"   Ø§Ù„Ù†Ù‚Ø§Ø·: {best[1]['score']}/100")
    print()
    
    # Ù†ØµØ§Ø¦Ø­ Ø¥Ø¶Ø§ÙÙŠØ©
    print("ğŸ’¡ Ù…Ù„Ø§Ø­Ø¸Ø§Øª:")
    
    if '_cleaned.txt' in best[0]:
        print("   âœ… Ù…Ù„ÙØ§Øª TXT Ø£ÙØ¶Ù„ Ù„Ù€ Gemini File Search:")
        print("      - Ø¨Ø­Ø« Ø£Ø³Ø±Ø¹ ÙˆØ£ÙƒØ«Ø± Ø¯Ù‚Ø©")
        print("      - Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø´Ø§ÙƒÙ„ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ")
        print("      - Ø­Ø¬Ù… Ø£ØµØºØ±")
    elif '_cleaned.pdf' in best[0]:
        print("   âœ… Ù…Ù„Ù PDF Ø§Ù„Ù†Ø¸ÙŠÙ ÙŠØ­ØªÙØ¸ Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚:")
        print("      - Ù…ÙÙŠØ¯ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ­ØªØ§Ø¬ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙˆØ§Ù„ØµÙˆØ±")
        print("      - Ù„ÙƒÙ† Ù…Ù„Ù TXT Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø£Ø³Ø±Ø¹ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«")
    elif '_ocr.pdf' in best[0]:
        print("   âš ï¸ Ù…Ù„Ù OCR Ø¬ÙŠØ¯ Ù„ÙƒÙ†:")
        print("      - Ù‚Ø¯ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ headers/footers Ù…ØªÙƒØ±Ø±Ø©")
        print("      - Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù†Ø¸ÙŠÙ (_cleaned) Ø£ÙØ¶Ù„")
    
    return sorted_scores

def main():
    """Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
    print("="*70)
    print("ØªØ­Ù„ÙŠÙ„ ÙˆÙ…Ù‚Ø§Ø±Ù†Ø© Ù…Ù„ÙØ§Øª PDF Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
    print("="*70)
    
    output_dir = Path("output")
    
    if not output_dir.exists():
        print("âŒ Ù…Ø¬Ù„Ø¯ output ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯!")
        return
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù„ÙØ§Øª PDF ÙˆØ§Ù„Ù†Øµ
    pdf_files = list(output_dir.glob("*.pdf"))
    txt_files = list(output_dir.glob("*.txt"))
    
    if not pdf_files and not txt_files:
        print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ output!")
        return
    
    print(f"\nâœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰:")
    print(f"   - {len(pdf_files)} Ù…Ù„Ù PDF")
    print(f"   - {len(txt_files)} Ù…Ù„Ù Ù†ØµÙŠ")
    
    # ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª PDF
    pdf_analyses = {}
    print(f"\nğŸ” ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª PDF...")
    for pdf_file in pdf_files:
        print(f"   Ù…Ø¹Ø§Ù„Ø¬Ø© {pdf_file.name}...")
        analysis = analyze_pdf(str(pdf_file))
        if analysis:
            key = pdf_file.stem
            pdf_analyses[key] = analysis
    
    # ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Øµ
    text_analyses = {}
    print(f"\nğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ©...")
    for txt_file in txt_files:
        if 'chunk_simulation' not in txt_file.name:
            print(f"   Ù…Ø¹Ø§Ù„Ø¬Ø© {txt_file.name}...")
            analysis = analyze_text_file(str(txt_file))
            if analysis:
                key = txt_file.stem
                text_analyses[key] = analysis
    
    # Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© ÙˆØ§Ù„ØªÙˆØµÙŠØ§Øª
    compare_pdfs(pdf_analyses)
    recommend_best_file(pdf_analyses, text_analyses)
    
    # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
    report = {
        'pdf_analyses': pdf_analyses,
        'text_analyses': text_analyses,
        'timestamp': str(Path.cwd())
    }
    
    report_path = output_dir / 'comparison_report.json'
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“Š ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„ÙƒØ§Ù…Ù„ ÙÙŠ: {report_path}")

if __name__ == '__main__':
    main()
